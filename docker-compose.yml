version: '3.8'

services:
  # Service MySQL (Datalake)
  mysql-dlk:
    image: mysql:latest
    container_name: mysql-dlk
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: securepassword
      MYSQL_DATABASE: datalakes_staging
      MYSQL_USER: datalakes_user
      MYSQL_PASSWORD: securepassword
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - dlk-network

  # Service MongoDB
  mongodb-dlk:
    image: mongo:latest
    container_name: mongodb-dlk
    restart: always
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - dlk-network

  # PostgreSQL pour Airflow
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - dlk-network

  # Initialisation de la base de donn√©es Airflow
  airflow-init:
    build:
      context: .
    image: airflow-custom
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      _AIRFLOW_DB_UPGRADE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: "airflow"
      _AIRFLOW_WWW_USER_PASSWORD: "airflow"
    command: "airflow db init"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./.env:/opt/airflow/.env
      - airflow_data:/opt/airflow
    networks:
      - dlk-network
    restart: "no"
    healthcheck:
      test: ["CMD", "airflow", "db", "check"]
      interval: 10s
      retries: 5
      start_period: 5s

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
    image: airflow-custom
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully  # Attendre la fin de airflow-init
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT: 300
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./.env:/opt/airflow/.env
      - airflow_data:/opt/airflow  # Partage le volume avec airflow-init
    ports:
      - "8080:8080"
    command: bash -c "/opt/airflow/scripts/init_webserver.sh"
    networks:
      - dlk-network

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
    image: airflow-custom
    container_name: airflow-scheduler
    depends_on:
      airflow-init :
        condition: service_completed_successfully  # Attendre la fin de airflow-init
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./.env:/opt/airflow/.env
      - airflow_data:/opt/airflow
    command: bash -c "/opt/airflow/scripts/init_scheduler.sh"
    networks:
      - dlk-network

  # API Gateway
  api-gateway:
    build:
      context: ./api
    container_name: api-gateway
    environment:
      - MYSQL_HOST=mysql-dlk
      - MYSQL_USER=datalakes_user
      - MYSQL_PASSWORD=securepassword
      - MYSQL_DATABASE=datalakes_staging
      - MYSQL_PORT=3306
      - MONGO_URI=mongodb://mongodb-dlk:27017/
      - MONGO_DATABASE=datalakes_curated
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - BUCKET_NAME=${BUCKET_NAME}
    ports:
      - "8000:8000"
    networks:
      - dlk-network
    depends_on:
      - mysql-dlk
      - mongodb-dlk
    command: bash -c "sleep 120 && uvicorn main:app --host 0.0.0.0 --port 8000"

volumes:
  mysql_data:
  mongodb_data:
  postgres_data:
  airflow_dags:
  airflow_logs:
  airflow_plugins:
  airflow_data:

networks:
  dlk-network:
    driver: bridge
